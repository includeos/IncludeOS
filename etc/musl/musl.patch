diff --git a/arch/i386/atomic_arch.h b/arch/i386/atomic_arch.h
index 7d2a48a..0d9fc0f 100644
--- a/arch/i386/atomic_arch.h
+++ b/arch/i386/atomic_arch.h
@@ -80,7 +80,7 @@ static inline void a_spin()
 #define a_crash a_crash
 static inline void a_crash()
 {
-	__asm__ __volatile__( "hlt" : : : "memory" );
+	__asm__ __volatile__( "ud2" : : : "memory" );
 }
 
 #define a_ctz_64 a_ctz_64
diff --git a/arch/x86_64/atomic_arch.h b/arch/x86_64/atomic_arch.h
index da4e203..08beb81 100644
--- a/arch/x86_64/atomic_arch.h
+++ b/arch/x86_64/atomic_arch.h
@@ -105,7 +105,7 @@ static inline void a_spin()
 #define a_crash a_crash
 static inline void a_crash()
 {
-	__asm__ __volatile__( "hlt" : : : "memory" );
+	__asm__ __volatile__( "ud2" : : : "memory" );
 }
 
 #define a_ctz_64 a_ctz_64
diff --git a/src/thread/pthread_cancel.c b/src/thread/pthread_cancel.c
index 3d22922..a560b4f 100644
--- a/src/thread/pthread_cancel.c
+++ b/src/thread/pthread_cancel.c
@@ -30,7 +30,7 @@ long __syscall_cp_c(syscall_arg_t nr,
 
 	if ((st=(self=__pthread_self())->canceldisable)
 	    && (st==PTHREAD_CANCEL_DISABLE || nr==SYS_close))
-		return __syscall(nr, u, v, w, x, y, z);
+		return syscall_n(nr, u, v, w, x, y, z);
 
 	r = __syscall_cp_asm(&self->cancel, nr, u, v, w, x, y, z);
 	if (r==-EINTR && nr!=SYS_close && self->cancel &&
diff --git a/src/unistd/setxid.c b/src/unistd/setxid.c
index 0239f8a..75c4165 100644
--- a/src/unistd/setxid.c
+++ b/src/unistd/setxid.c
@@ -13,7 +13,7 @@ static void do_setxid(void *p)
 {
 	struct ctx *c = p;
 	if (c->err>0) return;
-	int ret = -__syscall(c->nr, c->id, c->eid, c->sid);
+	int ret = -syscall_n(c->nr, c->id, c->eid, c->sid);
 	if (ret && !c->err) {
 		/* If one thread fails to set ids after another has already
 		 * succeeded, forcibly killing the process is the only safe
